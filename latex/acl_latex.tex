\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% Including images in your LaTeX document requires adding
% additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage[most]{tcolorbox} % 必须引入 most 以支持增强功能
\usepackage{enumitem}
\usepackage{xspace}

\setlist[itemize]{nolistsep,noitemsep,leftmargin=*}
\pgfplotsset{compat=1.16}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds}

% Define colors for figures
\definecolor{gpt5color}{RGB}{74,144,226}
\definecolor{deepseekcolor}{RGB}{80,200,120}
\definecolor{minimaxcolor}{RGB}{255,165,0}
\definecolor{kimicolor}{RGB}{186,85,211}
\definecolor{geminicolor}{RGB}{255,99,71}
\definecolor{gptosscolor}{RGB}{128,128,128}

\newtcolorbox{paperbox}[2][]{%
  enhanced,
  colback=white,
  colframe=black,
  fonttitle=\bfseries,
  before=\refstepcounter{table},
  title={#2},
  #1
}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  columns=fullflexible,
  keepspaces=true
}

\lstdefinelanguage{json}{
  morestring=[b]",
  morecomment=[l]{//},
  showstringspaces=false
}

\newcommand{\method}{MTRouter\xspace}
% \newcommand{\methodplus}{\textsc{VOLT+}}
\newcommand{\github}{\url{https://anonymous.4open.science/r/multi-turn-model-routing-infra-BF0F}\xspace}

\title{MTRouter: Cost-Aware Multi-Turn LLM Routing with History–Model Joint Embeddings}

\author{Anonymous Authors}

\begin{document}
\maketitle
	\begin{abstract}
	Multi-turn LLM agents solve tasks through many sequential interactions, where the total cost of repeated model calls becomes a first-class concern.
	We study \emph{multi-turn model routing}: selecting which model from a heterogeneous pool to invoke at each turn of an episode, conditioned on the evolving interaction history.
	We introduce \method, which learns an \emph{outcome estimator} over history--model pairs from logged trajectories and uses it to make turn-level routing decisions under a fixed evaluation budget.
	On ScienceWorld and HLE, \method consistently improves the performance--cost trade-off: on ScienceWorld (test) it outperforms GPT-5 while reducing total cost by 58.7\%, and on HLE (test) it improves accuracy while reducing total cost by 43.4\%; it also maintains strong gains under semantic distribution shift (OOD).
	Beyond aggregate results, we analyze why multi-turn routing helps: compared to Router-R1, \method reaches success with fewer model switches and is less reactive to transient errors, and it exhibits structured, benchmark-specific model usage and emergent specialization across tools/actions.
	Code and data are available at \github.
	\end{abstract}

\section{Introduction}
\input{sections/introduction}

\section{Related Work}
\input{sections/related_work}

\section{Method}
\label{sec:method}
\input{sections/method}

\section{Experiments}
\label{sec:experiments}
\input{sections/experiments}

\section{Conclusion}
\input{sections/conclusion}

\section*{Limitations}
\input{sections/limitations}

\bibliography{custom}

\appendix
\input{sections/appendix}

\end{document}
