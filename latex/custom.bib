% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

%%%%% zyq %%%
@article{hu2025hands,
  title={Hands-on LLM-based Agents: A Tutorial for General Audiences},
  author={Hu, Shuyue and Ren, Siyue and Chen, Yang and Mu, Chunjiang and Liu, Jinyi and Cui, Zhiyao and Zhang, Yiqun and Li, Hao and Zhou, Dongzhan and Xu, Jia and others},
  year={2025}
}
%%%%% zyq %%%
@article{wang2022scienceworld,
  title={Scienceworld: Is your agent smarter than a 5th grader?},
  author={Wang, Ruoyao and Jansen, Peter and C{\^o}t{\'e}, Marc-Alexandre and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2203.07540},
  year={2022}
}

@article{jimenez2024swebench,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{yao2022webshop,
  title={WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20744--20757},
  year={2022}
}

@article{ong2024routellm,
  title={Routellm: Learning to route llms with preference data},
  author={Ong, Isaac and Almahairi, Amjad and Wu, Vincent and Chiang, Wei-Lin and Wu, Tianhao and Gonzalez, Joseph E and Kadous, M Waleed and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.18665},
  year={2024}
}

@article{lu2023routing,
  title={Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models},
  author={Lu, Keming and Yuan, Hongyi and Lin, Runji and Lin, Junyang and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.08692},
  year={2023}
}

@article{chen2023frugalgpt,
  title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@article{wang2024embedllm,
  title={EmbedLLM: Learning Compact Representations of Large Language Models},
  author={Wang, Qianxin and Chen, Liqi and Chakraborty, Souradip and Dong, Qing and Zhang, Haoran and Liu, Yang and Qi, Zhenghai and Guo, Tianyi and Liu, Meng},
  journal={arXiv preprint arXiv:2401.11623},
  year={2024}
}

@article{zhang2025avengers,
  title={The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants},
  author={Zhang, Yiqun and Li, Hao and Wang, Chenxu and Chen, Linyao and Zhang, Qiaosheng and Ye, Peng and Feng, Shi and Wang, Daling and Wang, Zhen and Wang, Xinrun and others},
  journal={arXiv preprint arXiv:2505.19797},
  year={2025}
}

@article{chen2024routerdc,
  title={RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models},
  author={Chen, Shuhao and Wang, Weisen and Weng, Rongxin and Qi, Jiaxing},
  journal={arXiv preprint arXiv:2409.07518},
  year={2024}
}

@article{schick2023toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{qin2023toolllm,
  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@misc{openrouter,
  title={OpenRouter: Unified API for LLMs and Automatic Routing},
  author={{OpenRouter}},
  howpublished={\url{https://openrouter.ai}},
  year={2025},
  note={Accessed 2026-01-03}
}

@article{yao2023react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2023}
}

@article{yang2024sweagent,
  title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author={Yang, John and Jimenez, Carlos E and Wettig, Alexander and Liber, Kilian and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2405.15793},
  year={2024}
}

@article{zhou2024webarena,
  title={WebArena: A Realistic Web Environment for Building Autonomous Agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2024}
}

@inproceedings{leviathan2023speculative,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023}
}

@article{dohan2022cascade,
  title={Cascade: A Platform for Cost-Aware, Parallel Language Model Inference},
  author={Dohan, David and Norouzi, Mohammad and Wu, Yifei and Ariola, Andrius and Kim, Yonghui},
  journal={arXiv preprint arXiv:2211.11213},
  year={2022}
}

@article{shazeer2017moe,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022}
}

@article{levine2020offline,
  title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@inproceedings{kumar2020cql,
  title={Conservative Q-Learning for Offline Reinforcement Learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{kostrikov2021iql,
  title={Offline Reinforcement Learning with Implicit Q-Learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@inproceedings{li2010linucb,
  title={A Contextual-Bandit Approach to Personalized News Article Recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th International Conference on World Wide Web},
  pages={661--670},
  year={2010}
}

@inproceedings{agarwal2014contextual,
  title={Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits},
  author={Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
  booktitle={International Conference on Machine Learning},
  pages={1638--1646},
  year={2014}
}

@article{qwen2024,
  title={Qwen2.5 Technical Report},
  author={{Qwen Team}},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}
