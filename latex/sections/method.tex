% Method section

We present our framework for multi-turn model routing. We first formalize the problem (\S\ref{sec:formalization}), then describe our state and model representations (\S\ref{sec:representations}), and finally detail the learning approach (\S\ref{sec:learning}).

\subsection{Problem Formalization}
\label{sec:formalization}

\begin{figure*}[t]
\centering
\begin{tikzpicture}[
    node distance=0.6cm,
    block/.style={rectangle, draw, rounded corners, minimum width=2.0cm, minimum height=0.8cm, font=\small, align=center},
    encoder/.style={block, fill=blue!15},
    qnet/.style={block, fill=green!15},
    model/.style={block, fill=orange!15, minimum width=1.4cm},
    env/.style={block, fill=purple!15},
    data/.style={block, fill=yellow!15},
    arrow/.style={->, >=stealth, thick},
    darrow/.style={->, >=stealth, thick, dashed}
]

% Left: Trajectory Data Collection
\node[data, minimum width=2.5cm] (traj) {Trajectory Data\\$\{(h_t, a_t, S_{\text{final}}, c_t)\}$};
\node[above=0.3cm of traj, font=\footnotesize\bfseries] {Offline Collection};

% Center: Training Pipeline
\node[encoder, right=1.5cm of traj] (hist_enc) {History\\Encoder $\phi$};
\node[encoder, below=0.5cm of hist_enc] (model_enc) {Model\\Encoder $\psi$};
\node[qnet, right=1.2cm of hist_enc, yshift=-0.4cm] (qnet) {Value\\Predictor\\$\hat{s}, \hat{c}$};

% Model Pool (right side)
\node[model, right=1.2cm of qnet, yshift=0.8cm, fill=gpt5color!30] (m1) {Model 1};
\node[model, below=0.15cm of m1, fill=deepseekcolor!30] (m2) {Model 2};
\node[model, below=0.15cm of m2, fill=minimaxcolor!30] (m3) {Model 3};
\node[model, below=0.15cm of m3, fill=kimicolor!30] (m4) {Model 4};
\node[model, below=0.15cm of m4, fill=geminicolor!30] (m5) {Model 5};
\node[model, below=0.15cm of m5, fill=gptosscolor!30] (m6) {Model 6};
\node[above=0.2cm of m1, font=\footnotesize\bfseries] {Model Pool $\mathcal{A}$};

% Selection arrow
\node[right=0.2cm of qnet, yshift=-0.2cm] (select) {\footnotesize select $a_t$};

% Environment loop
\node[env, right=1.0cm of m3] (env) {Environment\\(Action, Obs)};

% Arrows - Training
\draw[arrow] (traj) -- (hist_enc);
\draw[arrow] (traj) |- (model_enc);
\draw[arrow] (hist_enc) -- (qnet);
\draw[arrow] (model_enc) -- (qnet);

% Arrows - Inference
\draw[arrow] (qnet) -- (select);
\draw[darrow] (select) -- (m3);
\draw[arrow] (m3) -- (env);
\draw[arrow, bend right=40] (env.south) to node[below, font=\tiny] {$h_{t+1}$} (hist_enc.south);

% Architecture details boxes
\node[below=1.5cm of hist_enc, draw, rounded corners, fill=white, font=\tiny, align=left, text width=2.8cm] (histdet) {
\textbf{History Encoder}\\
Task + recent turns\\
$\rightarrow z_x$
};

\node[below=1.5cm of qnet, draw, rounded corners, fill=white, font=\tiny, align=left, text width=2.8cm] (qdet) {
\textbf{Value Predictor}\\
Dual heads: $\hat{s}, \hat{c}$\\
$Q_\lambda = \hat{s} - \lambda \hat{c}$
};

\node[below=0.3cm of model_enc, draw, rounded corners, fill=white, font=\tiny, align=left, text width=2.8cm] (moddet) {
\textbf{Model Encoder}\\
Metadata + residual\\
$\rightarrow z_a$
};

% Dashed lines to detail boxes
\draw[dashed, gray] (hist_enc) -- (histdet);
\draw[dashed, gray] (qnet) -- (qdet);
\draw[dashed, gray] (model_enc) -- (moddet);

\end{tikzpicture}
\caption{\textbf{Overview of \method.} \textbf{Left:} Offline trajectories are collected under a uniform (random) router that samples models at each step. \textbf{Center:} The history encoder $\phi$ encodes the dialogue context into $z_x$; the model encoder $\psi$ maps each candidate to $z_a$. A dual-head value predictor estimates expected score $\hat{s}$ and cost $\hat{c}$. \textbf{Right:} At inference, the router selects $a^* = \arg\max_a Q_\lambda(h_t, a)$ where $Q_\lambda = \hat{s} - \lambda\hat{c}$, allowing deployment-time trade-off control.}
\label{fig:method_overview}
\end{figure*}

We view a multi-turn agent episode as a sequence of turns indexed by $t$. At each turn:

\begin{enumerate}
\item The agent observes \textbf{history} $h_t$, comprising the task description, dialogue context, and the most recent environment observation.
\item A \textbf{router} selects a model $a_t \in \mathcal{A}$ from the pool.
\item The selected model generates output $y_t \sim p_{a_t}(\cdot | h_t)$.
\item A \textbf{parser} extracts an environment action $u_t = \text{parse}(y_t)$.
\item The \textbf{environment} returns the next observation $o_{t+1}$ and indicates termination.
\end{enumerate}

The episode ends when the environment signals completion, a step limit is reached, or a cost budget is exhausted. The environment provides a terminal score $S_{\text{final}}$ (e.g., task completion percentage in ScienceWorld, or binary success in HLE).

\paragraph{Objective.}
We optimize a cost-adjusted objective with user-controllable trade-off:
\begin{equation}
J_\lambda = \mathbb{E}\left[S_{\text{final}} - \lambda \sum_{t=0}^{T-1} c_t\right]
\label{eq:objective}
\end{equation}
where $c_t$ is the cost at step $t$ (computed from token usage and model pricing), and $\lambda \geq 0$ controls the cost penalty. This Lagrangian formulation exposes the Pareto frontier: smaller $\lambda$ prioritizes performance, while larger $\lambda$ increasingly favors cheaper routing.

\subsection{State and Model Representations}
\label{sec:representations}

Effective routing requires meaningful representations of both the current interaction state and the candidate models.

\paragraph{History Encoding.}
We encode the interaction history into a fixed-dimensional vector $z_x=\phi(h_t)$ using a frozen text embedding model. The encoder summarizes the task description and recent observations/actions into a compact state representation for routing.

\paragraph{Model Encoding.}
Each candidate model $a \in \mathcal{A}$ is represented by a learned embedding $z_a=\psi(a)$ that combines (i) structured metadata (e.g., context limits and pricing) with (ii) a learned residual that captures model-specific behavior not explained by metadata.

The final model embedding concatenates these components through a linear projection:
\begin{equation}
z_a = W_{\text{proj}} \cdot [\text{MLP}(\text{attr}_a); e_a] + b_{\text{proj}}
\end{equation}
where $e_a$ is the residual embedding regularized to prevent overfitting.

\subsection{Learning a Routing Value Function}
\label{sec:learning}

We learn a routing value function that predicts the expected cost-adjusted outcome for each state--model pair. Rather than training separate routers for different $\lambda$ values, we decompose the prediction into score and cost components, enabling deployment-time adjustment.

\paragraph{Dual-Head Architecture.}
Given history encoding $z_x$ and model encoding $z_a$, we predict:
\begin{align}
\hat{s}(h_t, a) &= f_{\theta_s}(z_x, z_a) \approx \mathbb{E}[S_{\text{final}} | h_t, a] \\
\hat{c}(h_t, a) &= f_{\theta_c}(z_x, z_a) \approx \mathbb{E}[C_{t:\text{end}} | h_t, a]
\end{align}
where $C_{t:\text{end}} = \sum_{i=t}^{T-1} c_i$ is the remaining episode cost from step $t$. The cost-adjusted value is then:
\begin{equation}
Q_\lambda(h_t, a) = \hat{s}(h_t, a) - \lambda \cdot \hat{c}(h_t, a)
\end{equation}

\paragraph{Training Objective.}
We train on offline trajectories collected under a stochastic router. For each step $(h_t^{(k)}, a_t^{(k)})$ in trajectory $k$, the supervision targets are:
\begin{align}
y_s^{(k)} &= S_{\text{final}}^{(k)} \\
y_c^{(k)} &= \sum_{i=t}^{T_k-1} c_i^{(k)}
\end{align}

The loss combines score and cost prediction:
\begin{equation}
\mathcal{L} = \sum_{k,t} \left[ (\hat{s} - y_s)^2 + w_c \cdot (\hat{c} - y_c)^2 \right]
\end{equation}
where $w_c$ controls the relative weighting of cost prediction.

\paragraph{Data Collection.}
We collect trajectories under a \textbf{uniform (random) router} that samples models from the pool at each step. This provides broad coverage of model choices, which is important for offline learning.

\paragraph{Inference.}
At deployment, the router selects models greedily:
\begin{equation}
a_t^* = \arg\max_{a \in \mathcal{A}} Q_\lambda(h_t, a)
\end{equation}

The $\lambda$ parameter can be adjusted at inference time without retraining, allowing users to navigate the cost-quality trade-off based on their budget constraints.
