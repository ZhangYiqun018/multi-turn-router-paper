% Method section

We present our framework for multi-turn model routing. We first formalize the problem (\S\ref{sec:formalization}), then describe our state and model representations (\S\ref{sec:representations}), and finally detail the learning approach (\S\ref{sec:learning}).

\subsection{Problem Formalization}
\label{sec:formalization}

\begin{figure*}[t]
\centering
\begin{tikzpicture}[
    node distance=0.6cm,
    block/.style={rectangle, draw, rounded corners, minimum width=2.0cm, minimum height=0.8cm, font=\small, align=center},
    encoder/.style={block, fill=blue!15},
    qnet/.style={block, fill=green!15},
    model/.style={block, fill=orange!15, minimum width=1.4cm},
    env/.style={block, fill=purple!15},
    data/.style={block, fill=yellow!15},
    arrow/.style={->, >=stealth, thick},
    darrow/.style={->, >=stealth, thick, dashed}
]

% Left: Trajectory Data Collection
\node[data, minimum width=2.5cm] (traj) {Trajectory Data\\$\{(h_t, a_t, S_{\text{final}}, c_t)\}$};
\node[above=0.3cm of traj, font=\footnotesize\bfseries] {Offline Collection};

% Center: Training Pipeline
\node[encoder, right=1.5cm of traj] (hist_enc) {History\\Encoder $\phi$};
\node[encoder, below=0.5cm of hist_enc] (model_enc) {Model\\Encoder $\psi$};
\node[qnet, right=1.2cm of hist_enc, yshift=-0.4cm] (qnet) {Q-Network\\$\hat{s}, \hat{c}$};

% Model Pool (right side)
\node[model, right=1.2cm of qnet, yshift=0.8cm, fill=gpt5color!30] (m1) {GPT-5};
\node[model, below=0.15cm of m1, fill=deepseekcolor!30] (m2) {DeepSeek};
\node[model, below=0.15cm of m2, fill=minimaxcolor!30] (m3) {MiniMax};
\node[model, below=0.15cm of m3, fill=kimicolor!30] (m4) {Kimi-K2};
\node[model, below=0.15cm of m4, fill=geminicolor!30] (m5) {Gemini};
\node[model, below=0.15cm of m5, fill=gptosscolor!30] (m6) {GPT-OSS};
\node[above=0.2cm of m1, font=\footnotesize\bfseries] {Model Pool $\mathcal{A}$};

% Selection arrow
\node[right=0.2cm of qnet, yshift=-0.2cm] (select) {\footnotesize $\arg\max_a Q_\lambda$};

% Environment loop
\node[env, right=1.0cm of m3] (env) {Environment\\(Action, Obs)};

% Arrows - Training
\draw[arrow] (traj) -- (hist_enc);
\draw[arrow] (traj) |- (model_enc);
\draw[arrow] (hist_enc) -- (qnet);
\draw[arrow] (model_enc) -- (qnet);

% Arrows - Inference
\draw[arrow] (qnet) -- (select);
\draw[darrow] (select) -- (m3);
\draw[arrow] (m3) -- (env);
\draw[arrow, bend right=40] (env.south) to node[below, font=\tiny] {$h_{t+1}$} (hist_enc.south);

% Architecture details boxes
\node[below=1.5cm of hist_enc, draw, rounded corners, fill=white, font=\tiny, align=left, text width=2.8cm] (histdet) {
\textbf{History Encoder}\\
Qwen3-Embedding-0.6B\\
Task + Last $K$ turns\\
$\rightarrow z_x \in \mathbb{R}^{1024}$
};

\node[below=1.5cm of qnet, draw, rounded corners, fill=white, font=\tiny, align=left, text width=2.8cm] (qdet) {
\textbf{Q-Network}\\
MLP: [256, 256]\\
Dual heads: $\hat{s}, \hat{c}$\\
$Q_\lambda = \hat{s} - \lambda \hat{c}$
};

\node[below=0.3cm of model_enc, draw, rounded corners, fill=white, font=\tiny, align=left, text width=2.8cm] (moddet) {
\textbf{Model Encoder}\\
Attributes + Residual\\
$\rightarrow z_a \in \mathbb{R}^{64}$
};

% Dashed lines to detail boxes
\draw[dashed, gray] (hist_enc) -- (histdet);
\draw[dashed, gray] (qnet) -- (qdet);
\draw[dashed, gray] (model_enc) -- (moddet);

\end{tikzpicture}
\caption{\textbf{Overview of \method.} \textbf{Left:} Offline trajectories are collected under a roulette policy that randomly samples models at each step. \textbf{Center:} The history encoder $\phi$ encodes the dialogue context into $z_x$; the model encoder $\psi$ maps each candidate to $z_a$. A dual-head Q-network predicts expected score $\hat{s}$ and cost $\hat{c}$. \textbf{Right:} At inference, the router selects $a^* = \arg\max_a Q_\lambda(h_t, a)$ where $Q_\lambda = \hat{s} - \lambda\hat{c}$, allowing deployment-time trade-off control.}
\label{fig:method_overview}
\end{figure*}

We model multi-turn agent interaction as an episodic Markov Decision Process. At each step $t$:

\begin{enumerate}
\item The agent observes \textbf{history} $h_t$, comprising the task description, dialogue context, and the most recent environment observation.
\item A \textbf{router} $\pi$ selects a model $a_t \in \mathcal{A} = \{1, \ldots, m\}$ from the pool.
\item The selected model generates output $y_t \sim p_{a_t}(\cdot | h_t)$.
\item A \textbf{parser} extracts an environment action $u_t = \text{parse}(y_t)$.
\item The \textbf{environment} returns the next observation $o_{t+1}$ and indicates termination.
\end{enumerate}

The episode ends when the environment signals completion, a step limit is reached, or a cost budget is exhausted. The environment provides a terminal score $S_{\text{final}}$ (e.g., task completion percentage in ScienceWorld, or binary success in HLE).

\paragraph{Objective.}
We optimize a cost-adjusted objective with user-controllable trade-off:
\begin{equation}
J_\lambda(\pi) = \mathbb{E}_\pi\left[S_{\text{final}} - \lambda \sum_{t=0}^{T-1} c_t\right]
\label{eq:objective}
\end{equation}
where $c_t$ is the cost at step $t$ (computed from token usage and model pricing), and $\lambda \geq 0$ controls the cost penalty. This Lagrangian formulation exposes the Pareto frontier: at $\lambda=0$, we maximize performance regardless of cost; as $\lambda \to \infty$, we minimize cost.

\subsection{State and Model Representations}
\label{sec:representations}

Effective routing requires meaningful representations of both the current interaction state and the candidate models.

\paragraph{History Encoding.}
We encode the interaction history into a fixed-dimensional vector $z_x = \phi(h_t) \in \mathbb{R}^{1024}$ using a frozen text embedding model. Specifically, we use Qwen3-Embedding-0.6B \citep{qwen2024}, a decoder-only embedding model with 8K context support.

The history is segmented with priority: (1) task description, (2) most recent $K$ conversation turns, (3) remaining context up to the token budget. We use last-token pooling to obtain the final embedding. This encoding captures the semantic state of the task, recent actions, and observations that inform routing decisions.

\paragraph{Model Encoding.}
Each candidate model $a \in \mathcal{A}$ is represented by a learned embedding $z_a = \psi(a) \in \mathbb{R}^{64}$ constructed from two components:

\textbf{(1) Attribute features.} We extract five numerical attributes: maximum input tokens, maximum output tokens, input cost per million tokens, output cost per million tokens, and knowledge cutoff date. These are log-normalized and z-score standardized, then projected through an MLP to a 32-dimensional vector.

\textbf{(2) Residual embeddings.} Each model has a learnable 16-dimensional embedding initialized to zero, capturing model-specific effects not explained by metadata (e.g., instruction-following quality, reasoning style).

The final model embedding concatenates these components through a linear projection:
\begin{equation}
z_a = W_{\text{proj}} \cdot [\text{MLP}(\text{attr}_a); e_a] + b_{\text{proj}}
\end{equation}
where $e_a$ is the residual embedding with L2 regularization ($\alpha = 10^{-4}$) to prevent overfitting.

\subsection{Learning a Routing Value Function}
\label{sec:learning}

We learn a Q-function that predicts the expected cost-adjusted return for each state-model pair. Rather than training separate models for different $\lambda$ values, we decompose the prediction into score and cost components, enabling deployment-time adjustment.

\paragraph{Dual-Head Architecture.}
Given history encoding $z_x$ and model encoding $z_a$, we predict:
\begin{align}
\hat{s}(h_t, a) &= f_{\theta_s}(z_x, z_a) \approx \mathbb{E}[S_{\text{final}} | h_t, a] \\
\hat{c}(h_t, a) &= f_{\theta_c}(z_x, z_a) \approx \mathbb{E}[C_{t:\text{end}} | h_t, a]
\end{align}
where $C_{t:\text{end}} = \sum_{i=t}^{T-1} c_i$ is the remaining episode cost from step $t$. The Q-function is then:
\begin{equation}
Q_\lambda(h_t, a) = \hat{s}(h_t, a) - \lambda \cdot \hat{c}(h_t, a)
\end{equation}

The neural network backbone is a 2-layer MLP with hidden dimensions [256, 256], ReLU activations, and 0.1 dropout. The input is the concatenation $[z_x; z_a] \in \mathbb{R}^{1088}$.

\paragraph{Training Objective.}
We train on offline trajectories collected under a stochastic policy. For each step $(h_t^{(k)}, a_t^{(k)})$ in trajectory $k$, the supervision targets are:
\begin{align}
y_s^{(k)} &= S_{\text{final}}^{(k)} \\
y_c^{(k)} &= \sum_{i=t}^{T_k-1} c_i^{(k)}
\end{align}

The loss combines score and cost prediction:
\begin{equation}
\mathcal{L} = \sum_{k,t} \left[ (\hat{s} - y_s)^2 + w_c \cdot (\hat{c} - y_c)^2 \right]
\end{equation}
where $w_c$ is the cost loss weight (set to 0 in our experiments, as we found score prediction sufficient for effective routing).

\paragraph{Data Collection.}
We collect trajectories under a \textbf{roulette policy} that uniformly samples from the model pool at each step: $\mu(a|h_t) = 1/|\mathcal{A}|$. This ensures coverage over the action space, which is critical for offline learning.

For each task variation, we run 5 roulette episodes and 3 baseline episodes (single-model runs). The combined dataset contains 137K training samples across both benchmarks.

\paragraph{Inference.}
At deployment, the router selects models greedily:
\begin{equation}
a_t^* = \arg\max_{a \in \mathcal{A}} Q_\lambda(h_t, a)
\end{equation}

The $\lambda$ parameter can be adjusted at inference time without retraining, allowing users to navigate the cost-quality trade-off based on their budget constraints.

\subsection{Baseline Routers}
\label{sec:baselines}

We compare against several baseline routing strategies:

\textbf{Roulette.} Uniformly samples from the model pool at each step: $\pi(a|h_t) = 1/|\mathcal{A}|$. This matches the roulette collection policy used to gather offline trajectories.

\textbf{Single-Turn Routers (episode-level).} We include three single-turn routing methods---RouterDC \citep{chen2024routerdc}, EmbedLLM \citep{wang2024embedllm}, and Avengers \citep{lu2024avengers}---as \emph{per-episode} routers: at the beginning of an episode, the router consumes the initial query context (history) and selects a model $a$ from the pool; the agent then uses the same model for all subsequent turns in the episode. This baseline is intentionally stronger than a fixed single-model choice (it can tailor the model to the query), but it cannot adapt to within-episode phase changes or error recovery, thereby highlighting the need for multi-turn routing.

\textbf{Router-R1.} A multi-turn LLM router baseline that uses a policy LLM (DeepSeek-V3.2) to choose which model to invoke at each step based on the last 3 turns and model descriptions. We include the policy LLM cost when reporting total cost.

\textbf{LLM Router.} A multi-turn LLM router baseline that uses DeepSeek-V3 as the policy model. We use the same routing prompt/template as Router-R1; only the policy model is changed.

\textbf{OpenRouter.} A representative commercial routing solution \citep{openrouter} that uses OpenRouter's automatic routing API to select which model to invoke at each step. We treat the router as a black box and provide it with the same per-step history context used by other routing baselines. Notably, OpenRouter can route over a substantially larger candidate set than our fixed 6-model pool (Appendix~\ref{app:openrouter}), making this baseline strictly more powerful in terms of available model options.
