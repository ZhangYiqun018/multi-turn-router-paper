LLM agents are increasingly deployed to solve complex, tool-using tasks---from software engineering \citep{jimenez2024swebench} to scientific reasoning \citep{wang2022scienceworld} and web navigation \citep{yao2022webshop}.
These agentic workflows often require dozens of sequential model calls, so the total cost can quickly dominate.
At the same time, the ``model zoo'' now spans orders of magnitude in price and capability, raising a natural question: \emph{can an agent adaptively use expensive models only when needed, while relying on cheaper models for routine turns?}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{figures/toy-example.pdf}
\caption{\textbf{Top:} a single-turn (episode-level) router selects one model and keeps it fixed throughout the episode. \textbf{Middle:} a multi-turn router can adapt the model choice across turns based on the evolving interaction state. \textbf{Bottom:} \method achieves a better performance--cost trade-off than representative baselines on ScienceWorld (SW) and HLE.}
\label{fig:toy_example}
\end{figure}

This question is especially acute in long-horizon, interactive evaluations.
Many agent benchmarks allow dozens of steps or turns per episode (e.g., 100 steps in ScienceWorld) \citep{wang2022scienceworld}.
To approximate peak capability, some evaluations also consider essentially unrestricted regimes (e.g., ``unlimited-turn'' coding baselines), where the goal is to maximize task success even at high test-time cost \citep{gao2025lessempiricalstudyturncontrol}.
However, this practice can be prohibitively expensive: in multi-turn agent loops, the running context (trajectory, intermediate artifacts, tool outputs) is repeatedly carried forward, and total token consumption can grow superlinearly with the number of turns \citep{gao2025lessempiricalstudyturncontrol}.
This cost pressure is further amplified by frontier long-context models that encourage keeping very large contexts ``in the loop'' during exploration, and by the fact that per-call API cost scales with input and output tokens.
As a result, benchmarks often impose explicit step/cost caps or restricted evaluation protocols due to cost.
These constraints make cost-aware methods increasingly important for reproducible research.

Figure~\ref{fig:toy_example} highlights a key distinction.
\emph{Single-turn} routers make one model choice at the start of an episode and keep it fixed.
In contrast, we focus on \emph{multi-turn} routing, where the model choice can change across turns as the interaction evolves (e.g., stronger models for planning or recovery, cheaper models for routine tool use).
In our setting, a \textbf{turn} is one router decision followed by one invocation of the selected model to produce the agent's next output.

We propose \method, which learns an \emph{outcome estimator} from logged trajectories.
The estimator maps a history--model pair to an estimate of the eventual episode outcome (terminal score/accuracy), using an error-aware adjustment to provide a stable training signal from offline data.
At inference time, the router selects the candidate model with the highest predicted outcome at each turn, and the episode is evaluated under fixed cost and turn limits.

Empirically, \method delivers consistent gains in both performance and cost.
On ScienceWorld (test), it improves average score from 48.4 (GPT-5) to 53.8 while reducing total cost by 58.7\%; on HLE (test), it improves accuracy while reducing total cost by 43.4\%.
It also generalizes under semantic distribution shift (OOD), maintaining improved outcomes with substantial cost savings (Table~\ref{tab:combined_results}).
Beyond aggregate metrics, our analysis shows that multi-turn routing is not ``switch more'': \method reaches success with fewer model switches than Router-R1, is less reactive to transient errors, and exhibits structured model usage and emergent specialization across tools/actions (Figures~\ref{fig:cost_switches}--\ref{fig:action_by_model}).

Our key contributions are as follows:

\begin{itemize}[nolistsep, noitemsep, leftmargin=*]
\item We introduce \method, which learns an outcome estimator over history--model pairs from offline trajectories and performs turn-level routing in multi-turn agent episodes.
\item We evaluate on ScienceWorld and HLE (test and OOD) with a fixed candidate pool, and show consistent improvements in both performance and total cost over strong routing baselines, including Router-R1 and a representative commercial router.
\item We provide analyses that connect these gains to concrete routing behaviors, including fewer unnecessary switches, improved error recovery, and emergent specialization across tools/actions.
\end{itemize}

Code and data are available at \github.
