% Appendix

\section{Dataset and Split Details}
\label{app:data}

\subsection{ScienceWorld Task Types}

Table~\ref{tab:sw_tasks} lists the ScienceWorld task types used for training and out-of-distribution evaluation.

\input{tables/sw_tasks}

For each task type, we sample up to 30 variations (to bound collection time). Variations are split 60\%/20\%/20\% into train/validation/test using a fixed random seed (42).

\subsection{HLE Subject Categories}

\input{tables/hle_categories}

\section{Error Detection Rules}
\label{app:error_rules}

Table~\ref{tab:error_rules_detail} provides the full specification of error detection rules used to compute annealed error costs (AEC) during training.
Each rule consists of pattern strings matched against environment observations, a severity level (high/medium/low), and a description.

\paragraph{Design Principles.}
For HLE, we distinguish between model errors (format violations, Python exceptions) and external failures (HTTP errors, paywalls).
Model errors receive higher severity since they reflect controllable mistakes; external failures are marked low severity as they depend on third-party services.
For ScienceWorld, we only penalize truly invalid actions (commands not recognized by the environment parser).
Environmental feedback such as ``the door is not open'' or ``the object is already in your inventory'' represents normal exploration and is not treated as an error.

\paragraph{Severity Coefficients.}
Severity levels map to penalty coefficients in AEC: high (2.0), medium (1.0), low (0.25).
The warmup schedule uses $p_0{=}0.3$, $p_1{=}0.7$, $w_{\min}{=}0.3$, and $w_{\max}{=}1.0$.
These coefficients modulate the base penalty $1/N$ where $N$ is the expected episode length for the task category.

\input{tables/error_rules_detail}

\section{Training Dynamics}
\label{app:training}

\input{tables/r2_scores}

The value model achieves strong predictive performance, with score prediction $R^2$ ranging from 0.689 (Gemini) to 0.792 (GPT-5). Cost prediction is slightly lower, likely due to higher variance in remaining cost depending on success/failure paths.

\section{Additional Experiments}
\label{app:more_experiments}

\paragraph{Learned model embeddings.}
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/model_embeddings.png}
\caption{t-SNE visualization of learned model embeddings from the model encoder. The embeddings separate models by identity and form a clear cost-tier structure, with low-cost models (e.g., GPT-OSS, Gemini) distinct from higher-cost frontier models (e.g., GPT-5).}
\label{fig:model_embeddings}
\end{figure}

Figure~\ref{fig:model_embeddings} visualizes the learned model embeddings after training.
The encoder learns to distinguish the candidate models and organizes them by cost tier, suggesting it captures meaningful capability--cost structure beyond raw attributes.

\section{OpenRouter Baseline Details}
\label{app:openrouter}

\paragraph{Motivation.}
OpenRouter \citep{openrouter} provides an automatic routing API commonly used in commercial deployments.
We include OpenRouter as a representative commercial router baseline to contextualize \method against an off-the-shelf production routing system.

\paragraph{Model pool.}
Unlike our setting, which restricts routing to a fixed 6-model candidate pool (Table~\ref{tab:model_pool}), OpenRouter's automatic routing can choose from a much broader set of models.
In our evaluation, the OpenRouter baseline routes over the following pool (as reported by the API at routing time):

\input{tables/openrouter_pool}

\paragraph{Implications for comparison.}
Because OpenRouter can select from a superset of models (including multiple frontier and provider-specific options not present in our pool), it represents a stronger routing setting than ours.
We still include it as a practical reference point, but comparisons should be interpreted with this mismatch in candidate pools in mind.

\paragraph{Why does OpenRouter perform poorly on ScienceWorld?}
Figure~\ref{fig:openrouter_model_usage} shows the model usage distribution of OpenRouter on both benchmarks.
On ScienceWorld, OpenRouter predominantly selects lightweight models: Mistral-Nemo accounts for 68\% of all model calls, followed by GPT-5-nano (24\%) and Claude-4.5 (7\%).
These models, while cost-efficient, lack the reasoning capability required for ScienceWorld's procedural scientific tasks.
In contrast, on HLE, OpenRouter allocates 54\% of calls to Claude-4.5---a much stronger frontier model---along with Sonar (23\%) and Mistral-Nemo (15\%), reflecting a more appropriate difficulty assessment for that benchmark.

This discrepancy reveals a key limitation of general-purpose commercial routers: without task-specific training, they may underestimate the difficulty of unfamiliar domains and over-rely on cheaper models.
ScienceWorld's text-based interface and seemingly simple commands may mislead the router into treating it as an easy task, when in fact it requires multi-step planning and precise action sequencing that weaker models struggle to execute.
The resulting negative scores ($-26.4$ on Test, $-26.9$ on OOD) indicate that the selected models frequently fail to make meaningful progress toward task completion, as judged by the environment's scoring function.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\columnwidth]{figures/openrouter_model_usage.pdf}
\caption{Model usage distribution of OpenRouter on ScienceWorld and HLE. On ScienceWorld, OpenRouter predominantly selects lightweight models (Mistral-Nemo: 68\%, GPT-5-nano: 24\%), while on HLE it primarily uses stronger models (Claude-4.5: 54\%), reflecting different difficulty assessments.}
\label{fig:openrouter_model_usage}
\end{figure}

\section{Reproducibility}
\label{app:reproduce}

\paragraph{Code and Data.}
All code, trained models, and trajectory data will be released at \url{[anonymized for review]}. The framework is implemented in Python 3.10 with PyTorch 2.0.

\paragraph{Compute Requirements.}
\begin{itemize}
\item Trajectory collection: 6$\times$ A100 GPUs, approximately 48 hours
\item Embedding precomputation: 1$\times$ A100, approximately 4 hours
\item Router training: 1$\times$ A100, approximately 2 hours
\item Evaluation: 2$\times$ A100, approximately 8 hours per benchmark
\end{itemize}

\paragraph{API Costs.}
Total API cost for experiments: approximately \$3,500 USD across data collection and evaluation.

\section{Prompts and Tool Schemas}
\label{app:prompts}

We report the prompts used in our evaluation setup. For brevity, we omit the format-error correction prompts.

\subsection{ScienceWorld Agent Prompt}
\label{app:prompts:sw}
\input{prompts/scienceworld_system}
\input{prompts/scienceworld_instance}

\subsection{HLE Agent Prompt}
\label{app:prompts:hle}
The HLE system prompt injects tool schemas via a template variable. We list each tool schema explicitly in Appendix~\ref{app:tool_schemas}.
\input{prompts/hle_system}
\input{prompts/hle_instance}

\subsection{Tool Schemas}
\label{app:tool_schemas}
We list each tool's JSON schema (name, description, and parameter schema) used by the HLE agent.
\input{prompts/tool_schema_search_json}
\input{prompts/tool_schema_browse_json}
\input{prompts/tool_schema_python_json}
\input{prompts/tool_schema_answer_json}

\subsection{Routing Model Prompt for LLM Router / Router-R1}
\label{app:prompts:routing}
We report the turn-level routing prompt used by the LLM Router baseline and by Router-R1. In both baselines, the routing model is queried at each turn to produce a \texttt{<select>} decision. They use the same prompt template; Router-R1 uses a trained \texttt{Qwen2.5-7B-Instruct} routing model, while LLM Router directly uses \texttt{DeepSeek-V3.2} (no training).
\input{prompts/llmrouter_system}
\input{prompts/llmrouter_model_descriptors}

\subsection{Browse Extractor Prompt}
\label{app:prompts:browse}
The HLE \texttt{browse} tool can optionally use an LLM to extract and summarize relevant evidence from retrieved webpages. We report the extractor prompt used for this LLM-based summarization.
\input{prompts/browse_extractor_prompt}

\subsection{HLE Judge Prompt}
\label{app:prompts:judge}
HLE scoring uses an LLM-as-a-judge component aligned with the official HLE evaluation. We report the judge prompt used to compare a model response against the provided reference answer.
\input{prompts/hle_judge_prompt}
