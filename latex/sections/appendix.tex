% Appendix

\section{Dataset and Split Details}
\label{app:data}

\subsection{ScienceWorld Task Types}

Table~\ref{tab:sw_tasks} lists the ScienceWorld task types used for training and out-of-distribution evaluation.

\input{tables/sw_tasks}

For each task type, we sample up to 30 variations (to bound collection time). Variations are split 60\%/20\%/20\% into train/validation/test using a fixed random seed (42).

\subsection{HLE Subject Categories}

\input{tables/hle_categories}

\section{Error Analysis}
\label{app:errors}

\subsection{Error Type Distribution}

Table~\ref{tab:error_types} shows the distribution of errors across models on HLE.

\input{tables/error_types}

Format errors dominate (88.5\%), indicating that models frequently fail to produce correctly formatted tool calls. This motivates the router's strategy of escalating to stronger models when format errors occur.

\subsection{Error Rates by Model}

\input{tables/model_errors}

Interestingly, GPT-5's error rate (51.8\%) is higher than some cheaper models on HLE. This is because GPT-5 takes fewer steps (7.0 vs 26.0 for DeepSeek), so each step is more consequential. MiniMax-M2's high error rate (88.5\%) explains its poor standalone performance despite low cost.

\section{Model Embedding Visualization}
\label{app:embeddings}

Figure~\ref{fig:model_embeddings} shows UMAP visualization of learned model embeddings $z_a$.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{
\centering
\textit{[Model Embedding UMAP Visualization]}\\[2mm]
\small
The learned embeddings cluster models by capability tier:\\
- Premium cluster: GPT-5\\
- Mid-tier cluster: DeepSeek, Kimi\\
- Economy cluster: GPT-OSS, Gemini, MiniMax
}}
\caption{UMAP projection of learned model embeddings. Models cluster by capability/cost tier, with GPT-5 isolated as the premium option.}
\label{fig:model_embeddings}
\end{figure}

The embedding space learns to separate models by capability tier rather than provider, suggesting the residual embeddings capture performance characteristics beyond the raw attribute features.

\section{Training Dynamics}
\label{app:training}

\input{tables/r2_scores}

The Q-function achieves strong predictive performance, with score prediction $R^2$ ranging from 0.689 (Gemini) to 0.792 (GPT-5). Cost prediction is slightly lower, likely due to higher variance in remaining cost depending on success/failure paths.

\section{Prompts and Tool Schemas}
\label{app:prompts}

\subsection{ScienceWorld Agent Prompt}
\label{app:prompts:sw}

We report the ScienceWorld agent prompts used in our evaluation setup.
For brevity, we omit the format-error correction prompt and only list the system and instance templates.

\paragraph{System prompt.}
\input{prompts/scienceworld_system}

\paragraph{Instance prompt.}
\input{prompts/scienceworld_instance}

\subsection{HLE Agent Prompt}
\label{app:prompts:hle}

We report the HLE agent prompts used in our evaluation setup.
For brevity, we omit the format-error correction prompt and only list the system and instance templates.
The system prompt includes tool schemas via a template variable; we list each tool schema explicitly in Appendix~\ref{app:tool_schemas}.

\paragraph{System prompt.}
\input{prompts/hle_system}

\paragraph{Instance prompt.}
\input{prompts/hle_instance}

\subsection{Tool Schemas}
\label{app:tool_schemas}

The HLE agent injects XML tool schemas into the system prompt (see Appendix~\ref{app:prompts:hle}).
We list each tool schema (as rendered to XML) below.

\paragraph{\texttt{search}.}
\input{prompts/tool_schema_search_xml}

\paragraph{\texttt{browse}.}
\input{prompts/tool_schema_browse_xml}

\paragraph{\texttt{python}.}
\input{prompts/tool_schema_python_xml}

\paragraph{\texttt{answer}.}
\input{prompts/tool_schema_answer_xml}

\subsection{Routing Policy Prompt for LLM Router / Router-R1}
\label{app:prompts:routing}

We report the single-turn routing prompt used by the LLM Router baseline and by Router-R1.
Both use the same prompt template; they differ only in which policy LLM is used to produce the \texttt{<select>} decision.

\paragraph{System prompt.}
\input{prompts/llmrouter_system}

\paragraph{Model descriptors.}
The routing prompt instantiates \texttt{\{model\_list\}} using per-model descriptors; we list the descriptors used in our experiments below.
\input{prompts/llmrouter_model_descriptors}

\subsection{Browse Extractor Prompt}
\label{app:prompts:browse}

The HLE \texttt{browse} tool can optionally use an LLM to extract and summarize relevant evidence from retrieved webpages.
We report the extractor prompt used for this LLM-based summarization.

\input{prompts/browse_extractor_prompt}

\subsection{HLE Judge Prompt}
\label{app:prompts:judge}

HLE scoring uses an LLM-as-a-judge component aligned with the official HLE evaluation.
We report the judge prompt used to compare a model response against the provided reference answer.

\input{prompts/hle_judge_prompt}

\section{OpenRouter Baseline Details}
\label{app:openrouter}

\paragraph{Motivation.}
OpenRouter \citep{openrouter} provides an automatic routing API commonly used in commercial deployments.
We include OpenRouter as a representative commercial router baseline to contextualize \method against an off-the-shelf production routing system.

\paragraph{Model pool.}
Unlike our setting, which restricts routing to a fixed 6-model candidate pool (Table~\ref{tab:model_pool}), OpenRouter's automatic routing can choose from a much broader set of models.
In our evaluation, the OpenRouter baseline routes over the following pool (as reported by the API at routing time):

\input{tables/openrouter_pool}

\paragraph{Implications for comparison.}
Because OpenRouter can select from a superset of models (including multiple frontier and provider-specific options not present in our pool), it represents a stronger routing setting than ours.
We still include it as a practical reference point, but comparisons should be interpreted with this mismatch in candidate pools in mind.

\section{Sankey Diagram: Error Recovery Flow}
\label{app:sankey}

Figure~\ref{fig:sankey_description} describes the error recovery flow visualization.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\columnwidth}{
\centering
\textit{[Error Recovery Sankey Diagram]}\\[2mm]
\small
Flow from error-triggering model $\rightarrow$ recovery model $\rightarrow$ outcome:\\[1mm]
\textbf{ScienceWorld:}\\
- 16 error switches, 15 recover successfully (93.8\%)\\
- GPT-OSS $\rightarrow$ GPT-5: 100\% recovery\\
- MiniMax $\rightarrow$ GPT-5: 100\% recovery\\[2mm]
\textbf{HLE:}\\
- 250 error switches, 87 recover (34.8\%)\\
- Format errors most common trigger (233)\\
- Escalation to GPT-5 has 31.9\% accuracy
}}
\caption{Description of error recovery flow. On ScienceWorld, error escalation to premium models has near-perfect recovery; on HLE, recovery is more challenging due to task difficulty.}
\label{fig:sankey_description}
\end{figure}

\section{Reproducibility}
\label{app:reproduce}

\paragraph{Code and Data.}
All code, trained models, and trajectory data will be released at \url{[anonymized for review]}. The framework is implemented in Python 3.10 with PyTorch 2.0.

\paragraph{Compute Requirements.}
\begin{itemize}
\item Trajectory collection: 6$\times$ A100 GPUs, approximately 48 hours
\item Embedding precomputation: 1$\times$ A100, approximately 4 hours
\item Router training: 1$\times$ A100, approximately 2 hours
\item Evaluation: 2$\times$ A100, approximately 8 hours per benchmark
\end{itemize}

\paragraph{API Costs.}
Total API cost for experiments: approximately \$3,500 USD across data collection and evaluation.
