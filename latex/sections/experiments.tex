% Experiments section

We evaluate \method on two challenging multi-turn benchmarks and analyze the learned routing patterns.

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate on two diverse benchmarks:

\textbf{ScienceWorld} \citep{wang2022scienceworld} is a text-based interactive environment requiring procedural scientific reasoning. Tasks include boiling water, growing plants, testing conductivity, and Mendelian genetics experiments. The environment provides a terminal score $S_{\text{final}} \in [0, 100]$ based on task completion. We use 13 task types for training/validation with a 60\%/20\%/20\% variation split, reserving 12 task types for out-of-distribution testing. Episodes are limited to 50 steps and \$2.0 cost.

\textbf{HLE (Humanity's Last Exam)} is a challenging long-context benchmark spanning academic domains. Questions require multi-step reasoning with tool use (web search, Python execution, file reading). Success is binary based on answer correctness. We use 6 subject categories (Math, Physics, Chemistry, Biology/Medicine, Engineering, Computer Science/AI) for training, with 2 categories (Humanities/Social Science, Other) held out for OOD evaluation.

\paragraph{Model Pool.}
We evaluate with 6 frontier LLMs spanning a 100$\times$ cost range:

\input{tables/model_pool}

\paragraph{Training Details.}
We train for 100 epochs with early stopping (patience=3) using AdamW optimizer (lr=$10^{-3}$, weight decay=0.01) and cosine annealing. Batch size is 64. History embeddings are precomputed using vLLM for efficiency. Training takes approximately 2 hours on a single A100 GPU.

\paragraph{Baselines.}
We compare against: (1) \textbf{Single-model baselines}: each model used exclusively; (2) \textbf{Roulette}: uniform random selection at each step; (3) \textbf{Router-R1}: a multi-turn LLM router baseline; (4) \textbf{LLM Router}: a multi-turn LLM router baseline; (5) \textbf{OpenRouter}: a representative commercial router \citep{openrouter} using OpenRouter's automatic routing API; (6) \textbf{Step-wise single-turn routers}: RouterDC \citep{chen2024routerdc}, EmbedLLM \citep{wang2024embedllm}, and Avengers \citep{lu2024avengers} applied independently at each step. We emphasize that OpenRouter can route over a substantially larger candidate set than our fixed 6-model pool (Appendix~\ref{app:openrouter}), so it is advantaged in terms of available model options.

\subsection{Main Results}
\label{sec:main_results}

\input{tables/main_results}

Table~\ref{tab:main_results} presents our main results. Key findings:

\paragraph{Matching best single-model at lower cost.}
On ScienceWorld, GPT-5 achieves the strongest single-model performance (48.4 score). We compare routing methods against this oracle baseline while tracking cost and steps.

\paragraph{Outperforming naive routing.}
Roulette achieves strong performance (37.7 score on ScienceWorld; 24.2\% on HLE) by mixing model capabilities. In contrast, the commercial OpenRouter baseline performs poorly on ScienceWorld.

\paragraph{Cross-benchmark transfer.}
On HLE, LLM Router achieves 24.0\% accuracy, close to GPT-5 (25.1\%), while OpenRouter attains 18.3\%.

\subsection{Out-of-Distribution Generalization}

\input{tables/ood_results}

Table~\ref{tab:ood_results} reports OOD performance on held-out ScienceWorld task types and held-out HLE categories. We include OpenRouter as a strong practical baseline that is advantaged by a larger candidate pool (Appendix~\ref{app:openrouter}).

\subsection{Analysis}
\label{sec:analysis}

\begin{figure}[t]
\centering
\includegraphics[width=0.80\columnwidth]{figures/model_embeddings.png}
\caption{t-SNE visualization of learned model embeddings from the model encoder. The embeddings separate models by identity and form a clear cost-tier structure, with low-cost models (e.g., GPT-OSS, Gemini) distinct from higher-cost frontier models (e.g., GPT-5).}
\label{fig:model_embeddings}
\end{figure}

\paragraph{Learned model embeddings.}
Figure~\ref{fig:model_embeddings} visualizes the learned model embeddings after training.
The encoder learns to distinguish the candidate models and organizes them by cost tier, suggesting it captures meaningful capability--cost structure beyond raw attributes.

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.85]
\begin{axis}[
    ybar stacked,
    bar width=12pt,
    xlabel={Episode Phase},
    ylabel={Model Usage (\%)},
    symbolic x coords={Early (1-5), Mid (6-15), Late (16+)},
    xtick=data,
    ymin=0, ymax=100,
    legend style={at={(0.5,-0.25)}, anchor=north, legend columns=3, font=\tiny},
    enlarge x limits=0.25,
    width=0.95\columnwidth,
    height=5cm,
]
\addplot[fill=gpt5color!80] coordinates {(Early (1-5), 51.6) (Mid (6-15), 38.0) (Late (16+), 29.8)};
\addplot[fill=gptosscolor!80] coordinates {(Early (1-5), 19.0) (Mid (6-15), 24.0) (Late (16+), 28.1)};
\addplot[fill=geminicolor!80] coordinates {(Early (1-5), 10.0) (Mid (6-15), 14.0) (Late (16+), 15.8)};
\addplot[fill=deepseekcolor!80] coordinates {(Early (1-5), 7.7) (Mid (6-15), 10.0) (Late (16+), 8.8)};
\addplot[fill=kimicolor!80] coordinates {(Early (1-5), 6.3) (Mid (6-15), 8.0) (Late (16+), 10.5)};
\addplot[fill=minimaxcolor!80] coordinates {(Early (1-5), 5.4) (Mid (6-15), 6.0) (Late (16+), 7.0)};
\legend{GPT-5, GPT-OSS, Gemini, DeepSeek, Kimi, MiniMax}
\end{axis}
\end{tikzpicture}
\caption{Model usage distribution across episode phases on ScienceWorld. The router prefers GPT-5 (expensive) for early planning and shifts to cheaper models in later phases.}
\label{fig:phase_usage}
\end{figure}

\paragraph{Phase-dependent model selection.}
Figure~\ref{fig:phase_usage} shows model usage across episode phases. In early steps (1-5), the router heavily favors GPT-5 (51.6\%), which excels at initial planning. As episodes progress, usage shifts toward cheaper models: GPT-OSS increases from 19.0\% to 28.1\%, while GPT-5 decreases from 51.6\% to 29.8\%. This pattern reflects the insight that early decisions are more critical for task success.

\paragraph{Error-triggered model switching.}
We analyze model switches triggered by errors (format errors, invalid actions). On ScienceWorld, 4.1\% of switches follow errors, with 100\% of error-to-premium switches leading to eventual success. On HLE, 49.0\% of switches are error-triggered, with format errors being most common (233 instances). The router learns to ``escalate'' to stronger models when errors occur.

\input{tables/specialization}

\paragraph{Task-specific model specialization.}
Table~\ref{tab:specialization} reveals emergent specialization: Gemini-Flash is used 7$\times$ more for chemistry-mix tasks (perhaps benefiting from its large context), while GPT-5 dominates boiling tasks (3$\times$). GPT-OSS-120B specializes in identification tasks (lifespan, melt). This specialization emerges purely from learning, without explicit task-model assignments.

\paragraph{Q-value dynamics during switching.}
When the router switches models, Q-values consistently improve: the average Q-delta is +0.032 for error-triggered switches and +0.046 for non-error switches. Notably, 100\% of switches have positive Q-delta, indicating the router confidently believes switching improves expected outcomes.

\subsection{Ablation Studies}

\input{tables/ablation}

Table~\ref{tab:ablation} reports ablations on ScienceWorld; results are omitted here pending final runs.
