% Experiments section

We evaluate \method on two challenging multi-turn benchmarks and analyze the learned routing patterns.
Unless otherwise stated, we repeat each experiment three times and report the mean.

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate on two diverse benchmarks with both in-distribution (ID) and out-of-distribution (OOD) splits.
Because agentic systems are often deployed in open-ended settings, robustness to distribution shift is a primary concern; accordingly, we construct OOD evaluations that are \emph{semantically} disjoint from training and ID test (no overlap), rather than relying on random re-sampling.

\textbf{ScienceWorld} \citep{wang2022scienceworld} is a text-based interactive environment requiring procedural scientific reasoning. Tasks include boiling water, growing plants, testing conductivity, and Mendelian genetics experiments. The environment provides a terminal score $S_{\text{final}} \in [-100, 100]$ based on task completion. We use 13 task types for training/validation with a 60\%/20\%/20\% variation split, reserving 12 task types for out-of-distribution testing. Episodes are limited to 50 steps and \$2.0 cost.

\textbf{HLE (Humanity's Last Exam)} is a challenging long-context benchmark spanning academic domains. Questions require multi-step reasoning with tool use (web search, Python execution, file reading). Success is binary based on answer correctness. We use 6 subject categories (Math, Physics, Chemistry, Biology/Medicine, Engineering, Computer Science/AI) for training, with 2 categories (Humanities/Social Science, Other) held out for OOD evaluation.

\paragraph{Tool Configuration.}
\textbf{HLE.} We enable four tools: \texttt{search}, \texttt{browse}, \texttt{python}, and \texttt{answer}.
The \texttt{search} tool uses Serper's Google Search API to retrieve web results given a query.
The \texttt{browse} tool fetches webpage content (via Jina Reader when enabled) and, to avoid context overflow from long pages, truncates the retrieved content and uses a separate summarization model to extract goal-relevant evidence (Appendix~\ref{app:prompts:browse}).
We use \texttt{python} for deterministic computation and \texttt{answer} to submit the final response.
Tool schemas are injected into the HLE system prompt at runtime and are listed explicitly in Appendix~\ref{app:tool_schemas}.

\textbf{ScienceWorld.} ScienceWorld does not require external web tools.
We interact with the environment through a single text-action interface: at each turn, the agent outputs exactly one command in a \texttt{```text} block and receives an observation from the simulator.
The environment additionally provides built-in query commands (e.g., \texttt{?navigation}, \texttt{?object}) to enumerate valid actions without consuming a game turn, which we include as part of the default interaction protocol.

\paragraph{Error Detection.}
We detect errors from environment observations to compute annealed error costs during training.
Table~\ref{tab:error_categories} summarizes the error categories by benchmark.
HLE errors span format violations, Python execution failures, and tool-specific issues; ScienceWorld only penalizes unparseable actions (environmental feedback like ``door is not open'' is normal exploration, not an error).
Severity levels (high/medium/low) determine penalty coefficients in the AEC computation.
Full rule specifications are provided in Appendix~\ref{app:error_rules}.

\input{tables/error_categories}

\paragraph{Model Pool.}
We evaluate with 6 frontier LLMs spanning a 100$\times$ cost range:

\input{tables/model_pool}

To avoid ambiguity, throughout the paper ``Gemini-2.5-Flash-Lite'' refers to the specific model \texttt{google/gemini-2.5-flash-lite} (sometimes loosely called ``Gemini Flash'' in provider documentation).

\paragraph{Training Details.}
We train for 100 epochs with early stopping (patience=3) using AdamW optimizer (lr=$10^{-3}$, weight decay=0.01) and cosine annealing. Batch size is 64. History embeddings are precomputed using vLLM for efficiency. Training takes approximately 2 hours on a single A100 GPU.

\paragraph{Baselines.}
We compare against: (1) \textbf{Single-model baselines}: each model used exclusively; (2) \textbf{Random Router}: uniform random selection at each turn; (3) \textbf{Router-R1}: a turn-level multi-turn router where we train a \texttt{Qwen2.5-7B-Instruct} routing model following the Router-R1 recipe; (4) \textbf{LLM Router}: a turn-level multi-turn router using the same routing prompt as Router-R1 but directly using \texttt{DeepSeek-V3.2} as the routing model (no training); (5) \textbf{OpenRouter}: a representative commercial router \citep{openrouter} using OpenRouter's automatic routing API; (6) \textbf{Single-turn routers (episode-level)}: RouterDC \citep{chen2024routerdc}, EmbedLLM \citep{wang2024embedllm}, and AvengersPro (our implementation of Avengers \citep{zhang2025avengers}). For these single-turn routers, we route \emph{once} at the start of an episode based on the initial query context, then use the selected model for all subsequent turns. This design isolates the benefit of \emph{multi-turn} routing: if multi-turn adaptation matters, per-episode fixed selection should underperform routers that can switch models across turns. We include OpenRouter to contextualize \method against a widely deployed, off-the-shelf commercial routing system; however, its routing API does not allow us to customize the candidate model pool, and it can select from a substantially larger pool than our fixed 6-model setting (Appendix~\ref{app:openrouter}), which advantages OpenRouter in available model options.

\subsection{Results}
\label{sec:main_results}

\input{tables/combined_results}

\paragraph{Test.}
Table~\ref{tab:combined_results} reports both in-distribution (test) and out-of-distribution (OOD) results; we discuss the test columns here.
On ScienceWorld, \method achieves the best average score (53.8) while cutting total cost by 58.7\% vs.\ GPT-5; compared to Router-R1, it gains +11.7 points with 54.4\% lower total cost.
Episode-level routers (single-turn) that commit to one model per episode consistently lag behind, supporting the necessity of \emph{multi-turn} routing in interactive settings where phases and errors evolve over time.
Notably, OpenRouter produces negative scores on ScienceWorld because it underestimates task difficulty and over-relies on lightweight models (Appendix~\ref{app:openrouter}).
On HLE, \method attains the best accuracy (26.0\%) while remaining cost-efficient (43.4\% lower total cost than GPT-5 and 32.7\% lower than Router-R1); Router-R1 and LLM Router reach similar accuracy but at higher cost.
Overall, \method delivers a better accuracy--cost trade-off than both strong single-model baselines and existing routing baselines on both benchmarks.

\paragraph{OOD.}
We next examine the OOD columns of Table~\ref{tab:combined_results}, which evaluate semantic distribution shift (held-out ScienceWorld task types; held-out HLE subject categories).
On ScienceWorld OOD, \method improves over GPT-5 by +5.0 points while using 65.8\% lower total cost; on HLE OOD, it reaches 38.57\% accuracy with 52.3\% lower total cost than GPT-5.
These results show that \method not only generalizes under distribution shift but also preserves its budget efficiency, establishing the strongest overall performance among the compared methods.

\paragraph{Ablation Studies}

\input{tables/ablation}

Table~\ref{tab:ablation} reports ablations on ScienceWorld and HLE.
Replacing the MLP with a Ridge regressor consistently degrades performance, suggesting that non-linear feature fusion is important for accurate routing decisions.
Removing Random-Router trajectories from training also hurts, indicating that the mixed data improves coverage of failure modes and supports more robust routing.
We additionally ablate removing the error-penalty adjustment in the outcome target to isolate the contribution of this training signal.
Most notably, removing routing history (conditioning on only the current turn) causes a large drop on both benchmarks, confirming that effective multi-turn routing requires interaction context beyond the current step.
Finally, replacing the learned model encoder with hardcoded features leads to the largest degradation, supporting the claim that \method benefits from learning a task-adaptive representation of candidate models.

\subsection{Why Does \method Work?}
\label{sec:analysis}
While the in-domain and out-of-domain results (Table~\ref{tab:combined_results}) and ablations (Table~\ref{tab:ablation}) establish \method's effectiveness, we next ask a more diagnostic question: \emph{why} does it work?
We use a sequence of complementary analyses to connect the performance gains to concrete routing behaviors.

\paragraph{Start from a simple diagnostic: switching vs.\ cost.}
If multi-turn routing is ``just switch more often,'' then a router that switches frequently should dominate.
We find the opposite.
Figure~\ref{fig:cost_switches} plots, over \emph{successful} episodes, how cumulative API cost grows as the router makes additional model switches.
Each curve is constructed by replaying logged trajectories from \method and Router-R1, accumulating per-turn cost and counting switches along the episode.
Despite \method achieving better end performance (Table~\ref{tab:combined_results}), its trajectories typically reach success with \emph{fewer} switches and \emph{lower} cumulative cost (e.g., on ScienceWorld: $\sim$5 switches for \method vs.\ $\sim$20 for Router-R1).
Beyond differences in per-token pricing, frequent switching can also reduce the effectiveness of prompt caching in multi-turn settings, lowering cache hit rates and increasing the effective cost of serving long histories \citep{hu2025hands}.
This immediately raises a more specific question: \emph{when} does Router-R1 choose to switch, and are those switches actually helpful?

\begin{figure}[!htbp]
\centering
\includegraphics[width=\columnwidth]{figures/cost_switches.pdf}
\caption{Cumulative cost vs.\ cumulative model switches over successful episodes, comparing \method against Router-R1 on ScienceWorld and HLE (constructed by replaying logged trajectories).}
\label{fig:cost_switches}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\columnwidth]{figures/error_switch_recovery_2x1.pdf}
\caption{Error-triggered switching and recovery. Left: probability of switching models after an error (format errors / invalid actions). Right: probability that the next turn recovers (no error) conditioned on an error.}
\label{fig:error_switch_recovery}
\end{figure}

\paragraph{Switching less by being more patient around errors.}
Figure~\ref{fig:error_switch_recovery} shows that Router-R1 switches aggressively after errors, while \method is less reactive: it often keeps the current model and tries to continue.
Crucially, this is not ``ignoring'' errors: the right panel shows a higher probability of recovery on the next turn under \method.
Consistent with this, after an error \method stays with the same model $\approx$90.2\% of the time on ScienceWorld and $\approx$80.9\% on HLE, substantially higher than Router-R1 (38.3\% and 66.4\%, respectively).
Together with Figure~\ref{fig:cost_switches}, these trends suggest that \method avoids a large fraction of error-triggered switches that appear low-yield (e.g., reacting to transient formatting or action mistakes), helping control cumulative cost without sacrificing performance.
We hypothesize this gap stems from the learning signal: Router-R1 largely relies on a natural-language router prompt to infer when switching helps, whereas \method is trained directly on trajectory outcomes (terminal scores with annealed error penalties), providing more direct supervision for effective switching.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{figures/model_usage_by_turn.pdf}
\caption{Model usage by turn on ScienceWorld and HLE. \method exhibits structured, benchmark-specific routing behavior rather than uniformly switching models across turns.}
\label{fig:phase_usage}
\end{figure}

\paragraph{Not ``never switch''---but switch with structure.}
One might worry that the previous results simply reflect a conservative router that rarely changes models.
Figure~\ref{fig:phase_usage} rules this out: \method uses multiple models throughout an episode, but in a stable, benchmark-specific way rather than as a reflex to errors.
This suggests that the router is learning a \emph{strategy} (which models to rely on, and when), not just a generic ``upgrade on failure'' heuristic.
For instance, on ScienceWorld, GPT-5 accounts for 50.8\% of early turns, while GPT-OSS increases to 24.3\% in the final turns; in contrast, Router-R1 largely concentrates on DeepSeek and Gemini at roughly $\sim$45\% each across phases, exhibiting much less structured diversity.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=\linewidth]{figures/action_by_model_combined.pdf}
\caption{Tool/action specialization by model measured via \emph{lift} ($\mathrm{Lift}=\Pr(\text{model}\mid \text{tool})/\Pr(\text{model})$). Values $>1$ indicate a model is over-represented for a tool/action (specialization), while values $<1$ indicate under-use.}
\label{fig:action_by_model}
\end{figure*}

\paragraph{A concrete form of strategy: emergent specialization.}
To make this structure explicit, Figure~\ref{fig:action_by_model} measures \emph{lift}: how much a model is over-used for a tool/action relative to its overall frequency.
We observe clear specialization patterns (lift $>1$) that align with complementary strengths---e.g., on HLE, DeepSeek is over-represented on \texttt{search} (lift 1.66), GPT-5 on \texttt{python} (1.51), and Kimi on \texttt{browse} (1.98).
On ScienceWorld, we observe analogous specialization across action types, such as MiniMax on observation-heavy actions (1.62), Gemini on object interactions (1.58), and GPT-OSS on query commands (1.81).
These findings connect back to the main results: \method wins not by switching more, but by switching \emph{selectively} and assigning stable roles to models over the course of an episode.
Additional analysis of the model encoder (learned model embeddings) is provided in Appendix~\ref{app:more_experiments}.
