% Experiments section

We evaluate \method on two challenging multi-turn benchmarks and analyze the learned routing patterns.
Unless otherwise stated, we repeat each experiment three times and report the mean.

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate on two diverse benchmarks with both in-distribution (ID) and out-of-distribution (OOD) splits.
Because agentic systems are often deployed in open-ended settings, robustness to distribution shift is a primary concern; accordingly, we construct OOD evaluations that are \emph{semantically} disjoint from training and ID test (no overlap), rather than relying on random re-sampling.

\textbf{ScienceWorld} \citep{wang2022scienceworld} is a text-based interactive environment requiring procedural scientific reasoning. Tasks include boiling water, growing plants, testing conductivity, and Mendelian genetics experiments. The environment provides a terminal score $S_{\text{final}} \in [0, 100]$ based on task completion. We use 13 task types for training/validation with a 60\%/20\%/20\% variation split, reserving 12 task types for out-of-distribution testing. Episodes are limited to 50 steps and \$2.0 cost.

\textbf{HLE (Humanity's Last Exam)} is a challenging long-context benchmark spanning academic domains. Questions require multi-step reasoning with tool use (web search, Python execution, file reading). Success is binary based on answer correctness. We use 6 subject categories (Math, Physics, Chemistry, Biology/Medicine, Engineering, Computer Science/AI) for training, with 2 categories (Humanities/Social Science, Other) held out for OOD evaluation.

\paragraph{Tool Configuration.}
\textbf{HLE.} We enable four tools: \texttt{search}, \texttt{browse}, \texttt{python}, and \texttt{answer}.
The \texttt{search} tool uses Serper's Google Search API to retrieve web results given a query.
The \texttt{browse} tool fetches webpage content (via Jina Reader when enabled) and, to avoid context overflow from long pages, truncates the retrieved content and uses a separate summarization model to extract goal-relevant evidence (Appendix~\ref{app:prompts:browse}).
We use \texttt{python} for deterministic computation and \texttt{answer} to submit the final response.
Tool schemas are injected into the HLE system prompt at runtime and are listed explicitly in Appendix~\ref{app:tool_schemas}.

\textbf{ScienceWorld.} ScienceWorld does not require external web tools.
We interact with the environment through a single text-action interface: at each turn, the agent outputs exactly one command in a \texttt{```text} block and receives an observation from the simulator.
The environment additionally provides built-in query commands (e.g., \texttt{?navigation}, \texttt{?object}) to enumerate valid actions without consuming a game turn, which we include as part of the default interaction protocol.

\paragraph{Model Pool.}
We evaluate with 6 frontier LLMs spanning a 100$\times$ cost range:

\input{tables/model_pool}

To avoid ambiguity, throughout the paper ``Gemini-2.5-Flash-Lite'' refers to the specific model \texttt{google/gemini-2.5-flash-lite} (sometimes loosely called ``Gemini Flash'' in provider documentation).

\paragraph{Training Details.}
We train for 100 epochs with early stopping (patience=3) using AdamW optimizer (lr=$10^{-3}$, weight decay=0.01) and cosine annealing. Batch size is 64. History embeddings are precomputed using vLLM for efficiency. Training takes approximately 2 hours on a single A100 GPU.

\paragraph{Baselines.}
We compare against: (1) \textbf{Single-model baselines}: each model used exclusively; (2) \textbf{Random Router}: uniform random selection at each turn; (3) \textbf{Router-R1}: a turn-level multi-turn router where we train a \texttt{Qwen2.5-7B-Instruct} routing policy following the Router-R1 method; (4) \textbf{LLM Router}: a turn-level multi-turn router using the same routing prompt as Router-R1 but directly using \texttt{DeepSeek-V3.2} as the routing policy model (no training); (5) \textbf{OpenRouter}: a representative commercial multi-turn router \citep{openrouter} using OpenRouter's automatic routing API; (6) \textbf{Single-turn routers (episode-level)}: RouterDC \citep{chen2024routerdc}, EmbedLLM \citep{wang2024embedllm}, and AvengersPro (our implementation of Avengers \citep{lu2024avengers}). For these single-turn routers, we route \emph{once} at the start of an episode based on the initial query context, then use the selected model for all subsequent turns. This design isolates the benefit of \emph{multi-turn} routing: if multi-turn adaptation matters, per-episode fixed selection should underperform routers that can switch models across turns. We emphasize that OpenRouter can route over a substantially larger candidate set than our fixed 6-model pool (Appendix~\ref{app:openrouter}), so it is advantaged in terms of available model options.

\subsection{Main Results}
\label{sec:main_results}

\input{tables/main_results}

Table~\ref{tab:main_results} presents our main results. Key findings:

\paragraph{Improving performance under budget.}
On ScienceWorld, \method achieves the best score (53.8) while costing less than half of GPT-5 (\$5.74 vs \$13.91 total).
On HLE, \method also achieves the strongest accuracy (26.0\%), outperforming GPT-5 (25.1\%) while reducing total cost (\$34.97 vs \$61.77).

\paragraph{Multi-turn routing vs. episode-level selection.}
Episode-level single-turn routers (RouterDC, EmbedLLM, AvengersPro) can adapt the \emph{initial} model choice to the query, but they cannot switch models across turns.
In contrast, turn-level multi-turn routers can react to phase changes and recover from errors; \method consistently outperforms both episode-level routers and the Random Router baseline on ScienceWorld.

\paragraph{Comparison to turn-level router baselines.}
On HLE, Router-R1 (trained \texttt{Qwen2.5-7B-Instruct} router) and LLM Router (untrained \texttt{DeepSeek-V3.2} router) achieve comparable accuracies (24.2\% and 24.0\%), but both are more expensive than \method in total cost. OpenRouter achieves 18.3\% accuracy with substantially higher total cost.

\subsection{Out-of-Distribution Generalization}

\input{tables/ood_results}

Table~\ref{tab:ood_results} reports OOD performance on held-out ScienceWorld task types and held-out HLE categories. We include OpenRouter as a strong practical baseline that is advantaged by a larger candidate pool (Appendix~\ref{app:openrouter}). For readability, we report raw OOD metrics; an alternative presentation (common in robustness reporting) is to additionally include generalization gaps, e.g., $\Delta$Score = Score$_{\text{OOD}}$ $-$ Score$_{\text{ID}}$ and $\Delta$Acc = Acc$_{\text{OOD}}$ $-$ Acc$_{\text{ID}}$, to directly quantify degradation under distribution shift.

\subsection{Analysis}
\label{sec:analysis}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{figures/model_embeddings.png}
\caption{t-SNE visualization of learned model embeddings from the model encoder. The embeddings separate models by identity and form a clear cost-tier structure, with low-cost models (e.g., GPT-OSS, Gemini) distinct from higher-cost frontier models (e.g., GPT-5).}
\label{fig:model_embeddings}
\end{figure}

\paragraph{Learned model embeddings.}
Figure~\ref{fig:model_embeddings} visualizes the learned model embeddings after training.
The encoder learns to distinguish the candidate models and organizes them by cost tier, suggesting it captures meaningful capability--cost structure beyond raw attributes.

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.85]
\begin{axis}[
    ybar stacked,
    bar width=12pt,
    xlabel={Episode Phase},
    ylabel={Model Usage (\%)},
    symbolic x coords={Early (1-5), Mid (6-15), Late (16+)},
    xtick=data,
    ymin=0, ymax=100,
    legend style={at={(0.5,-0.25)}, anchor=north, legend columns=3, font=\tiny},
    enlarge x limits=0.25,
    width=0.95\columnwidth,
    height=5cm,
]
\addplot[fill=gpt5color!80] coordinates {(Early (1-5), 51.6) (Mid (6-15), 38.0) (Late (16+), 29.8)};
\addplot[fill=gptosscolor!80] coordinates {(Early (1-5), 19.0) (Mid (6-15), 24.0) (Late (16+), 28.1)};
\addplot[fill=geminicolor!80] coordinates {(Early (1-5), 10.0) (Mid (6-15), 14.0) (Late (16+), 15.8)};
\addplot[fill=deepseekcolor!80] coordinates {(Early (1-5), 7.7) (Mid (6-15), 10.0) (Late (16+), 8.8)};
\addplot[fill=kimicolor!80] coordinates {(Early (1-5), 6.3) (Mid (6-15), 8.0) (Late (16+), 10.5)};
\addplot[fill=minimaxcolor!80] coordinates {(Early (1-5), 5.4) (Mid (6-15), 6.0) (Late (16+), 7.0)};
\legend{GPT-5, GPT-OSS, Gemini, DeepSeek, Kimi, MiniMax}
\end{axis}
\end{tikzpicture}
\caption{Model usage distribution across episode phases on ScienceWorld. The router prefers GPT-5 (expensive) for early planning and shifts to cheaper models in later phases.}
\label{fig:phase_usage}
\end{figure}

\paragraph{Phase-dependent model selection.}
Figure~\ref{fig:phase_usage} shows model usage across episode phases. In early steps (1-5), the router heavily favors GPT-5 (51.6\%), which excels at initial planning. As episodes progress, usage shifts toward cheaper models: GPT-OSS increases from 19.0\% to 28.1\%, while GPT-5 decreases from 51.6\% to 29.8\%. This pattern reflects the insight that early decisions are more critical for task success.

\paragraph{Error-triggered model switching.}
We analyze model switches triggered by errors (format errors, invalid actions). On ScienceWorld, 4.1\% of switches follow errors, with 100\% of error-to-premium switches leading to eventual success. On HLE, 49.0\% of switches are error-triggered, with format errors being most common (233 instances). The router learns to ``escalate'' to stronger models when errors occur.

\input{tables/specialization}

\paragraph{Task-specific model specialization.}
Table~\ref{tab:specialization} reveals emergent specialization: Gemini-2.5-Flash-Lite is used 7$\times$ more for chemistry-mix tasks (perhaps benefiting from its large context), while GPT-5 dominates boiling tasks (3$\times$). GPT-OSS-120B specializes in identification tasks (lifespan, melt). This specialization emerges purely from learning, without explicit task-model assignments.

\paragraph{Q-value dynamics during switching.}
When the router switches models, Q-values consistently improve: the average Q-delta is +0.032 for error-triggered switches and +0.046 for non-error switches. Notably, 100\% of switches have positive Q-delta, indicating the router confidently believes switching improves expected outcomes.

\subsection{Ablation Studies}

\input{tables/ablation}

Table~\ref{tab:ablation} reports ablations on ScienceWorld; results are omitted here pending final runs.
