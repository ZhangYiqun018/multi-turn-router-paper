% Experiments section

We evaluate \method on two challenging multi-turn benchmarks and analyze the learned routing patterns.
Unless otherwise stated, we repeat each experiment three times and report the mean.

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate on two diverse benchmarks with both in-distribution (ID) and out-of-distribution (OOD) splits.
Because agentic systems are often deployed in open-ended settings, robustness to distribution shift is a primary concern; accordingly, we construct OOD evaluations that are \emph{semantically} disjoint from training and ID test (no overlap), rather than relying on random re-sampling.

\textbf{ScienceWorld} \citep{wang2022scienceworld} is a text-based interactive environment requiring procedural scientific reasoning. Tasks include boiling water, growing plants, testing conductivity, and Mendelian genetics experiments. The environment provides a terminal score $S_{\text{final}} \in [-100, 100]$ based on task completion. We use 13 task types for training/validation with a 60\%/20\%/20\% variation split, reserving 12 task types for out-of-distribution testing. Episodes are limited to 50 steps and \$2.0 cost.

\textbf{HLE (Humanity's Last Exam)} is a challenging long-context benchmark spanning academic domains. Questions require multi-step reasoning with tool use (web search, Python execution, file reading). Success is binary based on answer correctness. We use 6 subject categories (Math, Physics, Chemistry, Biology/Medicine, Engineering, Computer Science/AI) for training, with 2 categories (Humanities/Social Science, Other) held out for OOD evaluation.

\paragraph{Tool Configuration.}
\textbf{HLE.} We enable four tools: \texttt{search}, \texttt{browse}, \texttt{python}, and \texttt{answer}.
The \texttt{search} tool uses Serper's Google Search API to retrieve web results given a query.
The \texttt{browse} tool fetches webpage content (via Jina Reader when enabled) and, to avoid context overflow from long pages, truncates the retrieved content and uses a separate summarization model to extract goal-relevant evidence (Appendix~\ref{app:prompts:browse}).
We use \texttt{python} for deterministic computation and \texttt{answer} to submit the final response.
Tool schemas are injected into the HLE system prompt at runtime and are listed explicitly in Appendix~\ref{app:tool_schemas}.

\textbf{ScienceWorld.} ScienceWorld does not require external web tools.
We interact with the environment through a single text-action interface: at each turn, the agent outputs exactly one command in a \texttt{```text} block and receives an observation from the simulator.
The environment additionally provides built-in query commands (e.g., \texttt{?navigation}, \texttt{?object}) to enumerate valid actions without consuming a game turn, which we include as part of the default interaction protocol.

\paragraph{Model Pool.}
We evaluate with 6 frontier LLMs spanning a 100$\times$ cost range:

\input{tables/model_pool}

To avoid ambiguity, throughout the paper ``Gemini-2.5-Flash-Lite'' refers to the specific model \texttt{google/gemini-2.5-flash-lite} (sometimes loosely called ``Gemini Flash'' in provider documentation).

\paragraph{Training Details.}
We train for 100 epochs with early stopping (patience=3) using AdamW optimizer (lr=$10^{-3}$, weight decay=0.01) and cosine annealing. Batch size is 64. History embeddings are precomputed using vLLM for efficiency. Training takes approximately 2 hours on a single A100 GPU.

\paragraph{Baselines.}
We compare against: (1) \textbf{Single-model baselines}: each model used exclusively; (2) \textbf{Random Router}: uniform random selection at each turn; (3) \textbf{Router-R1}: a turn-level multi-turn router where we train a \texttt{Qwen2.5-7B-Instruct} routing model following the Router-R1 recipe; (4) \textbf{LLM Router}: a turn-level multi-turn router using the same routing prompt as Router-R1 but directly using \texttt{DeepSeek-V3.2} as the routing model (no training); (5) \textbf{OpenRouter}: a representative commercial router \citep{openrouter} using OpenRouter's automatic routing API; (6) \textbf{Single-turn routers (episode-level)}: RouterDC \citep{chen2024routerdc}, EmbedLLM \citep{wang2024embedllm}, and AvengersPro (our implementation of Avengers \citep{lu2024avengers}). For these single-turn routers, we route \emph{once} at the start of an episode based on the initial query context, then use the selected model for all subsequent turns. This design isolates the benefit of \emph{multi-turn} routing: if multi-turn adaptation matters, per-episode fixed selection should underperform routers that can switch models across turns. We include OpenRouter to contextualize \method against a widely deployed, off-the-shelf commercial routing system; however, its routing API does not allow us to customize the candidate model pool, and it can select from a substantially larger pool than our fixed 6-model setting (Appendix~\ref{app:openrouter}), which advantages OpenRouter in available model options.

\subsection{Main Results}
\label{sec:main_results}

\input{tables/main_results}

Table~\ref{tab:main_results} presents our main results. Key findings:

\paragraph{Improving performance under budget.}
On ScienceWorld, \method achieves the best score (53.8) while costing less than half of GPT-5 (\$5.74 vs \$13.91 total).
On HLE, \method also achieves the strongest accuracy (26.0\%), outperforming GPT-5 (25.1\%) while reducing total cost (\$34.97 vs \$61.77).

\paragraph{Multi-turn routing vs. episode-level selection.}
Episode-level single-turn routers (RouterDC, EmbedLLM, AvengersPro) can adapt the \emph{initial} model choice to the query, but they cannot switch models across turns.
In contrast, turn-level multi-turn routers can react to phase changes and recover from errors; \method consistently outperforms both episode-level routers and the Random Router baseline on ScienceWorld.

\paragraph{Comparison to turn-level router baselines.}
On HLE, Router-R1 (trained \texttt{Qwen2.5-7B-Instruct} router) and LLM Router (untrained \texttt{DeepSeek-V3.2} router) achieve comparable accuracies (24.2\% and 24.0\%), but both are more expensive than \method in total cost. OpenRouter achieves 18.3\% accuracy with substantially higher total cost.

\subsection{Out-of-Distribution Generalization}

\input{tables/ood_results}

Table~\ref{tab:ood_results} reports OOD performance on held-out ScienceWorld task types and held-out HLE categories. We include OpenRouter as a strong practical baseline that is advantaged by a larger candidate pool (Appendix~\ref{app:openrouter}). For readability, we report raw OOD metrics; an alternative presentation (common in robustness reporting) is to additionally include generalization gaps, e.g., $\Delta$Score = Score$_{\text{OOD}}$ $-$ Score$_{\text{ID}}$ and $\Delta$Acc = Acc$_{\text{OOD}}$ $-$ Acc$_{\text{ID}}$, to directly quantify degradation under distribution shift.

\subsection{Analysis}
\label{sec:analysis}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{figures/model_embeddings.png}
\caption{t-SNE visualization of learned model embeddings from the model encoder. The embeddings separate models by identity and form a clear cost-tier structure, with low-cost models (e.g., GPT-OSS, Gemini) distinct from higher-cost frontier models (e.g., GPT-5).}
\label{fig:model_embeddings}
\end{figure}

\paragraph{Learned model embeddings.}
Figure~\ref{fig:model_embeddings} visualizes the learned model embeddings after training.
The encoder learns to distinguish the candidate models and organizes them by cost tier, suggesting it captures meaningful capability--cost structure beyond raw attributes.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\columnwidth]{figures/model_usage_by_turn.pdf}
\caption{Model usage by turn on ScienceWorld and HLE. The router adopts different turn-level selection strategies across benchmarks: ScienceWorld routing is relatively stable with GPT-5 dominating overall, while HLE uses more GPT-5 early and shifts toward cheaper models in later turns.}
\label{fig:phase_usage}
\end{figure}

\paragraph{Phase-dependent model selection.}
Figure~\ref{fig:phase_usage} shows model usage across turns, revealing distinct routing strategies across benchmarks. On ScienceWorld, routing is relatively stable: GPT-5 remains the dominant choice, with a mild mid-episode dip and higher usage at the beginning and end of episodes (to plan and to finish tasks). On HLE, the router relies more on GPT-5 in early turns but increasingly transitions to cheaper models (e.g., DeepSeek and MiniMax) in later turns. A plausible driver is that many HLE instances solvable by GPT-5 terminate within $\sim$10 turns, so the remaining long-horizon episodes benefit from cost-conscious switching once the heavy reasoning is done.

\paragraph{Error-triggered model switching.}
We analyze model switches triggered by errors (format errors, invalid actions). On ScienceWorld, 4.1\% of switches follow errors, with 100\% of error-to-premium switches leading to eventual success. On HLE, 49.0\% of switches are error-triggered, with format errors being most common (233 instances). The router learns to ``escalate'' to stronger models when errors occur.

\paragraph{Value estimates during switching.}
When the router switches models, its learned value estimates improve: the average value delta is +0.032 for error-triggered switches and +0.046 for non-error switches. Notably, 100\% of switches have positive value delta, suggesting the router can identify situations where switching is likely to improve outcomes.

\subsection{Ablation Studies}

\input{tables/ablation}

Table~\ref{tab:ablation} reports ablations on ScienceWorld; results are omitted here pending final runs.
